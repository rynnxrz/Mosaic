Mosaic: Product Requirements Document & Implementation Plan (iOS, 2025)1. OverviewThis document outlines the product requirements and provides a detailed implementation plan for the Mosaic iOS application, targeting development in 2025 using Xcode 16.3. It serves as a technical guide for the development team, detailing the application's vision, features, user experience, architecture, and development approach.1.1. Problem Statement & App VisionProblem: Digital memories—photos, videos, notes, audio clips—are often stored chronologically or thematically, detached from the rich context of the physical spaces and specific objects that originally evoked them. Existing platforms lack the capability to deeply embed these memories within a spatial representation of an environment, losing the powerful connection between place, object, and emotion. Furthermore, experiencing shared memories often lacks a sense of presence or the ability to understand the original creator's perspective.App Vision: Mosaic aims to bridge the physical and digital memory gap. It allows users to capture 3D scans of real-world interior spaces and attach multimedia "postings" or emotional imprints directly onto specific locations or objects within these digital replicas. The core vision is to provide an evocative and engaging experience where digital memories gain profound spatial context. Users can precisely define how their memories are presented, including capturing and sharing their original viewpoint at the moment of creation. For those exploring these spaces, Mosaic offers a journey of discovery through layered, subtly presented memories. By allowing explorers to temporarily adopt the creator's viewpoint, the app fosters a deeper empathetic connection to the memory, the place, and the shared human experience, all while respecting user privacy and encouraging playful, intuitive exploration.1.2. Target Users & Value PropositionThe application caters to distinct user roles, each deriving unique value:
Space Creator/Admin (User A): These users are typically individuals or professionals (e.g., homeowners, exhibition curators, event organizers, real estate agents) who wish to digitally capture, preserve, and potentially share the essence of a physical space.

Value Proposition: Provides a streamlined process using RoomPlan 1 to generate 3D models of interior spaces. Offers simple tools for saving, managing, and controlling the shareability of these spatial canvases, establishing a foundation for collective or personal memory layering.


Memory Creator (User B): These are individuals interacting with a scanned space (their own or one shared with them) who want to anchor personal or shared memories—stories, notes, media, emotional reflections—to specific points or objects within that space.

Value Proposition: Offers a novel and deeply contextual way to journal, reminisce, or share experiences. The ability to precisely place memories, define their visual presentation (e.g., the orientation of a "floating" photo), and crucially, capture and associate their own viewpoint 3 provides unparalleled expressive power and contextual richness.


Explorer (User C): These users navigate scanned spaces (personal or shared) to discover and experience the memories layered within them, potentially remotely.

Value Proposition: Delivers an interactive, engaging, and unique "sand-table" exploration experience. Facilitates the discovery of collective or personal histories embedded in a space. Offers the profound ability to step into the Memory Creator's shoes by adopting their saved viewpoint, fostering empathy and a richer understanding of the memory's context. Promotes connection to place and shared stories through subtle, evocative discovery mechanics.


1.3. Core Technology Stack SummaryThe Mosaic application will leverage modern Apple frameworks and a cloud backend:
Frontend UI: SwiftUI will be used exclusively for building the user interface programmatically. This ensures a modern, declarative approach, aligning with current iOS development best practices and facilitating maintainable view structures.5 No Storyboards will be used.
AR & 3D Scene Management: RealityKit serves as the primary framework for rendering 3D scenes (both the scanning interface augmentation and the exploration view), managing virtual objects (memory representations), handling interactions, and controlling camera perspectives.6 ARKit provides the foundational world-tracking, environmental understanding, and camera management capabilities that RealityKit builds upon.
Room Scanning: The RoomPlan API will be utilized for capturing 3D models of interior rooms. This framework leverages device sensors (camera, LiDAR) and machine learning to identify room layouts, geometry, surfaces, and common objects like furniture.1
Data Persistence & Backend: A cloud-based backend service (detailed in Section 4.3, with CloudKit recommended) is required for storing user data, scan data (USDZ models, RoomPlan JSON metadata 7), AR persistence data (ARWorldMap 9), memory content (multimedia files, text, metadata), and managing sharing permissions.
Development Environment: Xcode 16.3, Swift programming language.
The choice of SwiftUI promotes cleaner architecture, potentially avoiding issues like "Massive View Controllers" by encouraging separation of concerns, as discussed in relation to MVC patterns.5 RealityKit's advanced features, such as physically based rendering (PBR), custom shaders, and animation capabilities 6, are well-suited for realizing the visually rich and atmospheric elements described in the app flow, like subtle memory cues and mood shifts.2. Core FeaturesThis section details the primary functionalities of the Mosaic application.2.1. Space Scanning & Management (RoomPlan Integration)
What it does: Enables Space Creators/Admins (or any user for personal spaces) to generate 3D models of interior rooms using the device's camera and LiDAR scanner.
Why it's important: This feature creates the fundamental spatial canvas—the digital twin of the physical space—upon which all subsequent memory creation and exploration activities depend. It's the entry point for bringing real-world context into the app.
How it works at a high level:

Initiation: The user initiates a scan from a SwiftUI interface.
Guided Scanning: The app presents RoomPlan's RoomCaptureView, an AR view that guides the user through the scanning process. This view provides real-time visual feedback, graphic overlays on physical structures, and instructions to ensure comprehensive capture.1
Data Processing: During the scan, RoomPlan utilizes the device's camera feed, LiDAR data, and internal ML models to detect and analyze the room's structure (walls, floors, ceilings, openings) and identify common objects (furniture types like chairs, tables, beds; appliances like refrigerators).1
Output Generation: Upon scan completion, RoomPlan processes the captured data. It outputs a parametric 3D model, typically exported as a USDZ file suitable for rendering in RealityKit. Alongside the 3D geometry, it provides structured data, often exported as JSON, containing details about the room's layout, dimensions, and identified objects (including their category, dimensions, and position/orientation via bounding boxes).1 The system will derive a 2.5D top-down representation from this model for the exploration view.
Saving & Sharing: The generated USDZ model and JSON metadata are saved, initially locally. The user can name the scan and has the option to upload it to the backend service, making it persistent and potentially shareable based on chosen settings.


Object Identification Details: RoomPlan identifies objects based on categories (e.g., CapturedRoom.Object.Category.chair, CapturedRoom.Object.Category.table) and provides associated attributes (e.g., ChairType.stool, TableShapeType.rectangular) along with their bounding boxes and transforms.11 It does not provide the unique, detailed mesh geometry of each specific object instance within the room. This distinction is critical: users can attach memories to "the couch" as identified by RoomPlan, interacting with its general representation, but not to a specific cushion or armrest based on its fine-grained geometry captured during the scan. Interaction will likely involve selecting the object's bounding box or a representative icon derived from it.
2.2. Memory Creation & Annotation
What it does: Allows Memory Creators (User B) to access a loaded spatial model (either their own scan or a shared one) and attach multimedia memories to specific locations or identified objects within that space.
Why it's important: This is the core content creation engine of Mosaic. It empowers users to infuse the digital representation of a space with personal narratives, shared experiences, and emotional context, transforming the static 3D model into a living repository of memories.
How it works at a high level:

Context: The user has loaded a spatial model into the app's 3D/AR view.
Navigation & Pinpointing: The user navigates the 3D model. To anchor a memory, they can either:

Select an Object: Tap on the representation of an object identified by RoomPlan (interacting with its bounding box or icon). The system associates the memory with the object's identifier provided in the RoomPlan JSON data.11
Select a Location: Tap on a specific point on a surface (wall, floor, furniture surface) within the 3D model. This requires performing a raycast from the tap location on the screen into the 3D scene to find the intersection point with the RoomPlan USDZ mesh.13 The resulting 3D coordinate becomes the memory's anchor point.


Define Presentation:

Height Specificity: The user interface allows specifying the vertical position (height) of the memory anchor relative to the floor or the selected surface/object.
Memory Volume Transform: For visual memories (photos, videos), the user positions and orients a virtual "floating screen" or volume in the 3D space relative to the anchor point. This involves interactive controls (e.g., drag handles, rotation gizmos) within the 3D view.


** Record Creator's Viewpoint:** At the moment the user confirms the memory placement, the system automatically captures the current state of the device's camera within the AR world coordinate system. This involves retrieving the ARCamera's transform property, which is a simd_float4x4 matrix encoding both the camera's 3D position and its orientation.3 This transform matrix is saved as metadata alongside the memory content.
Select Content Type & Input: The user chooses the type of memory (Text, Audio, Photo, Video Clip) and inputs or captures the content using standard iOS interfaces (keyboard, microphone, camera roll, camera).
Set Privacy: The user selects the privacy level for this specific memory: Private (only visible to the creator), Shared with Specific Users (requires future implementation of user accounts and sharing logic), or Public within this Space (visible to anyone accessing this specific shared space, with an option for the creator to remain anonymous).
Save Memory: The memory (content or reference to media, anchor location/object ID, presentation transform, creator's viewpoint transform matrix, privacy setting, timestamp) is saved to the backend data store.


Viewpoint Significance: Capturing the creator's viewpoint transform is a defining feature. It allows Explorers to temporarily see the memory's anchor point from the exact perspective of the person who created it, significantly enhancing contextual understanding and empathy. Implementing this requires careful storage of the 4x4 matrix and applying it correctly to the Explorer's camera in the RealityKit scene during playback.
2.3. 2.5D Sand-Table Exploration
What it does: Provides Explorers (User C) with a unique interface for navigating scanned spaces and discovering embedded memories remotely, using a top-down or angled "sand-table" perspective.
Why it's important: Offers an intuitive, engaging, and less potentially disorienting alternative to first-person AR navigation for exploring spaces when not physically present. The 2.5D perspective emphasizes overview, discovery, and the relationship between memories within the space.
How it works at a high level:

Space Loading: The user selects and loads a personal or shared space.
View Initialization: The main screen area ("upper window") displays the 2.5D view, rendered using RealityKit. This view shows the RoomPlan model from a fixed top-down or high-angle perspective. Subtle, non-intrusive visual cues (e.g., soft light pulses, gentle color shifts, atmospheric effects) appear on or near locations/objects that contain memories.
Avatar Representation: A simple avatar representing the Explorer (User C) appears within the 2.5D scene.
Navigation Controls:

Avatar Movement: A virtual joystick, implemented as a SwiftUI view 16 located on the "lower screen" area, controls the avatar's translation (forward/backward, left/right) on the 2.5D plane.
Camera Following: The RealityKit camera's position generally follows the avatar, keeping it centered or within the primary view area.
Camera Orientation (Orbit): The user manually controls the camera's horizontal viewing angle (orbiting around the avatar) by swiping left or right on the upper screen area (the RealityKit view). The camera orientation does not automatically change based on the avatar's direction of movement.
Camera Pitch (Vertical Angle): The camera's vertical angle (pitch) is fixed or has very limited adjustment. This maintains the consistent 2.5D "sand-table" perspective, preventing users from looking straight down or too far towards the horizon. This might be achieved using RealityKit's OrthographicCameraComponent for a true parallel projection 17 or a PerspectiveCameraComponent positioned high above the scene with a fixed downward angle and constrained pitch controls. Prototyping will determine the best approach for visual clarity and the desired feel.


Memory Discovery:

Proximity Trigger: As the user's avatar moves close to a memory cue or interacts with a cued object (e.g., taps its representation), immediate subtle feedback occurs (e.g., a soft sound, a visual ripple effect).
Preview: A snippet or preview of the memory (e.g., first line of text, thumbnail of a photo) gently fades into view near the cue. Visual memories (photos/videos) may appear as small "floating screens" integrated into the scene.
Atmospheric Cues: The local ambiance around the cue might subtly shift (visual tone, background audio hints) to reflect the memory's nature or emotion. If multiple memories exist at one point, cues might become more intense or change color to indicate density.


Engaging Full Memory & Viewpoint Adoption:

Activation: The user taps the snippet or floating screen to view the full memory content.
Viewpoint Transition: The app offers to (or automatically, based on settings) temporarily transition the Explorer's RealityKit camera to the exact position and orientation stored in the memory's creator viewpoint transform matrix. This provides the "seeing through the creator's eyes" experience.
Return: After closing the full memory view, the camera smoothly transitions back to the Explorer's previous sand-table viewpoint and orientation.


Discovery Persistence: Once a memory has been fully viewed (potentially including the viewpoint adoption), its cue or floating screen might remain persistently visible (perhaps with a different visual state, e.g., "unlocked") for that session or longer, aiding recall and preventing redundant discovery unless desired.


2.4. Sharing & Privacy Controls
What it does: Provides users with control over the visibility and accessibility of their scanned spaces and the individual memories they create within them.
Why it's important: Essential for building user trust and supporting diverse use cases, ranging from private spatial journaling to public virtual exhibitions or shared family memory spaces. Granular control empowers users to manage their digital footprint appropriately.
How it works at a high level:

Space-Level Sharing: Space Creators/Admins determine the accessibility of their scanned spaces. Options include:

Private: Only the creator can access the scan and its associated memories.
Shareable Link/QR Code: The creator can generate a unique link or QR code. Anyone with this link/code can load the space and view memories marked as "Public within this Space." (Requires backend support for managing these links and associating them with space IDs).
Public Directory (Future): A potential future enhancement could allow creators to list their spaces in a discoverable directory within the app.


Memory-Level Privacy: When creating a memory, the Memory Creator assigns a privacy setting:

Private: Only the Memory Creator can view this memory, even within a shared space.
Shared with Specific Users (Future): Visible only to a specified list or group of other Mosaic users. This requires a robust user account system, identity management, and potentially friend/group functionality in the backend – deferred beyond MVP.
Public within this Space: Visible to anyone who has access to the containing space (via link, QR code, or future directory). An option for the creator to post anonymously might be included.


Backend Enforcement: The backend service is responsible for storing these permissions and enforcing them. When a user requests to load a space or its memories, the backend filters the results based on the user's identity and the defined privacy settings for the space and each individual memory.


Implementation Complexity: Implementing "Shared with Specific Users" introduces significant complexity compared to simple private/public models. It necessitates features like user accounts, authentication, authorization checks (potentially using Access Control Lists - ACLs), and possibly a social graph or contact system. For the initial phases (MVP), focusing on "Private" and "Public within this Space" (accessible via a shareable link) is recommended to manage scope effectively.
Table 1: Core Feature Breakdown
Feature NameDescription (What it does)User Value (Why it's important)High-Level Implementation Notes/Key TechnologiesSpace Scanning & ManagementGenerates 3D models of interior rooms using the device camera/LiDAR. Allows saving and managing these scans.Creates the foundational spatial canvas for memories. Enables digital preservation of physical spaces.RoomPlan (RoomCaptureView), SwiftUI, USDZ/JSON export 1, Backend Storage (e.g., CloudKit).Memory Creation & AnnotationAttaches multimedia memories (text, photo, video, audio) to specific locations or objects within a scanned space.Imbues spaces with personal/shared meaning. Allows precise contextual anchoring and expression via creator viewpoint capture.RealityKit (for placement view), Raycasting/Object Selection, ARCamera.transform capture 3, Multimedia Input, Backend Storage.2.5D Sand-Table ExplorationProvides a unique top-down/angled view for navigating spaces and discovering memories remotely.Offers an intuitive, engaging, and less disorienting way to explore spatial memories. Facilitates discovery and empathy.RealityKit (ARView, Orthographic/Constrained Perspective Camera 17), SwiftUI (Joystick 16, UI Overlays), Viewpoint Adoption Logic.Sharing & Privacy ControlsAllows users to control the visibility of scanned spaces and individual memories (private, shared link, public in space).Essential for user trust, control over personal data, and supporting various private/public use cases.Backend logic (Permissions/ACLs), User Authentication, UI Toggles for settings.
3. User Experience (UX)This section details the intended user experience, focusing on the target personas, their interaction flows, and key UI/UX considerations for a SwiftUI-based implementation.3.1. User Personas (Detailed Roles)Understanding the distinct needs and goals of each user persona is crucial for designing an intuitive and valuable experience.
Space Creator/Admin (User A):

Needs & Goals: Requires a simple, fast, and reliable process for scanning rooms. Needs clear visual feedback during the scan to ensure adequate coverage. Wants straightforward options for saving, naming, and managing multiple scans. Needs unambiguous controls for setting the sharing status of a space (private vs. shareable).
App Mapping: The app must provide a streamlined entry point to the RoomPlan scanning flow.1 Real-time progress indicators and clear instructions within RoomCaptureView are essential. Post-scan, the interface should offer simple options for naming the scan, viewing a preview, saving it (locally and/or to the cloud), and accessing sharing settings (e.g., generate link/QR code).


Memory Creator (User B):

Needs & Goals: Needs intuitive tools to navigate the 3D representation of the scanned space to find the perfect spot for a memory. Requires precise methods for anchoring memories, whether to a specific 3D coordinate or a recognized object. Wants control over the visual presentation of the memory (especially for photos/videos – position, orientation). Critically, needs the app to capture their viewpoint accurately.3 Expects support for various media types (text, audio, photo, video). Requires clear, granular options for setting memory privacy.
App Mapping: The memory creation interface must provide easy-to-use 3D navigation controls (e.g., orbit, pan, zoom) for the placement phase. Selection tools should clearly differentiate between tapping on a surface (triggering raycasting) and tapping on a recognized object's representation. Interactive gizmos or intuitive gestures should allow users to position and orient the "floating screen" for visual media. The viewpoint capture should be automatic upon confirmation or require a simple confirmation step. Standard iOS interfaces should be used for content input. Privacy settings must be clearly presented as toggles or selection options before saving.


Explorer (User C):

Needs & Goals: Desires an engaging and non-overwhelming way to discover hidden memories within a space. Needs intuitive navigation controls for the 2.5D sand-table view. Requires clear yet subtle cues indicating the presence and location of memories. Values the ability to experience a memory from the creator's original perspective to gain deeper context and connection. Wants a sense of progression and discovery while exploring.
App Mapping: The 2.5D sand-table view is the primary interface. It must feature the defined avatar/joystick/swipe control scheme. Visual cues for memories should be noticeable but aesthetically integrated (e.g., light pulses, atmospheric effects leveraging RealityKit 6). Interaction feedback (sound, ripples) should confirm proximity/selection. The transition to and from the creator's viewpoint must be smooth and contextually clear. Displaying discovered memories persistently helps track progress and avoids redundant interactions.


Table 2: User Persona Needs & App Mapping
User PersonaKey Needs/GoalsCorresponding App Features/UX ElementsSpace Creator/AdminSimple & reliable scanning; Clear scan feedback; Easy save/manage scans; Control over sharing.Streamlined RoomPlan flow 1, RoomCaptureView feedback, Simple save/name UI, Clear sharing options (private/link/QR).Memory CreatorIntuitive 3D navigation for placement; Precise memory anchoring (location/object); Define memory presentation; Capture creator viewpoint 3; Rich media support; Granular privacy.3D view navigation controls, Raycasting & Object selection UI, Memory volume transform gizmos/gestures, Automatic/confirmed viewpoint capture, Multimedia input views, Clear privacy toggles.ExplorerEngaging discovery; Intuitive 2.5D navigation; Clear memory cues; Experience creator's perspective; Sense of discovery & connection.2.5D Sand-Table view, Avatar + Joystick/Swipe controls 16, Subtle visual/atmospheric cues 6, Smooth viewpoint transition logic, Persistence of discovered memories.
3.2. Key User Flows (Scan-to-Explore Lifecycle)These flows illustrate the primary journeys users will take through the application.
Flow 1: Space Creation & Initial Memory Posting (User A & B)

Initiate Scan: User A opens Mosaic and selects "Create New Space."
Guided Scan: The app presents the RoomCaptureView. User A follows on-screen prompts to scan the room thoroughly.1
Process & Save: Scan completes. RoomPlan processes data, generating USDZ and JSON. User A names the scan (e.g., "Living Room - May 2025") and saves it (locally, with option to upload to backend).
Load & Navigate: User B (or User A) later opens the saved "Living Room" scan. The 3D model loads in a placement view. User B navigates the model.
Pinpoint & Define: User B decides to place a memory on the fireplace mantel (an object RoomPlan might identify) or a specific spot on the wall (using raycasting). They adjust the anchor height and, if placing a photo, position/orient its "floating screen."
Capture Viewpoint & Content: User B confirms placement. The app captures the ARCamera.transform.3 User B types a text note or selects a photo.
Set Privacy & Save: User B sets privacy to "Public within this Space" and saves the memory. Data is uploaded to the backend.


Flow 2: Exploring a Shared Space & Discovering Memories (User C)

Access Space: User C receives a link or scans a QR code for a shared space (e.g., "Community Art Exhibit"). The app uses the link/code to identify and load the correct space data from the backend.
Load 2.5D View: The "Community Art Exhibit" model loads into the 2.5D sand-table view. Subtle light pulses indicate memory locations. User C's avatar appears.
Navigate: User C uses the lower-screen joystick 16 to move their avatar towards a pulsing light near a specific sculpture. They swipe on the upper screen to orbit the camera for a better view.
Discover Preview: As the avatar nears the cue, a soft chime sounds, and a thumbnail preview of a photo memory fades in.
Engage Full Memory: User C taps the thumbnail. The app displays the full photo and associated text. Simultaneously, the 2.5D camera smoothly transitions to the creator's saved viewpoint, showing the sculpture from the angle they saw it when placing the memory.
Return to Exploration: User C closes the memory view. The camera transitions back to their previous sand-table position and orientation. The memory cue near the sculpture now appears slightly differently (e.g., brighter or a checkmark) indicating it has been discovered. User C continues exploring.


3.3. UI/UX Implementation Considerations (SwiftUI Focus)The user interface will be developed entirely using SwiftUI, adhering to modern iOS design principles.
SwiftUI First Principle: All user interface components, layouts, and navigation will be implemented programmatically using SwiftUI. This avoids reliance on Interface Builder (Storyboards, XIBs) and promotes a declarative, state-driven UI architecture, which generally leads to more maintainable and testable code.5 UIKit elements will only be used if absolutely necessary to wrap a critical component unavailable in SwiftUI, but the strong preference is for a pure SwiftUI approach.
AR View Integration: The core AR and 3D rendering capabilities provided by RealityKit's ARView will be integrated into the SwiftUI view hierarchy using the UIViewRepresentable protocol. A custom UIViewRepresentable struct will manage the lifecycle of the ARView instance and facilitate communication between the UIKit-based ARView and the surrounding SwiftUI environment.19 This is the standard approach for embedding complex UIKit views (like ARView) within SwiftUI layouts. While RealityView exists, particularly for visionOS 20, UIViewRepresentable wrapping ARView remains a common and flexible pattern for standard iOS screen-based AR.
2.5D View Design & Interaction:

Layout: A clear visual hierarchy will separate the upper "sand-table" view (the ARView container) from the lower control area containing SwiftUI elements like the joystick and memory interaction buttons.
Controls: The joystick will be a custom SwiftUI component, potentially based on existing examples 16, providing continuous updates of the user's desired movement vector. Standard SwiftUI gesture recognizers (DragGesture) applied to the ARView container in SwiftUI will capture swipe inputs for camera orbiting.
Feedback: Interactions should be reinforced with appropriate feedback. Haptic feedback (using UIImpactFeedbackGenerator or newer SwiftUI APIs) should accompany key actions like memory discovery or placement confirmation. Visual feedback for memory cues (pulses, ripples, atmospheric shifts) will be implemented within the RealityKit scene itself, potentially leveraging custom materials or shaders for unique effects.6


Memory Creation UI: Designing intuitive controls for placing and orienting the "floating screen" representations of visual memories in 3D is crucial. This might involve implementing custom 3D gizmos (translation/rotation handles) directly in the RealityKit scene or mapping touch gestures (drag, pinch, rotate) within the ARView to manipulate the selected memory entity's transform.
Information Hierarchy: Memory previews (snippets) should be concise and clearly associated with their spatial cue. The full memory view should present content clearly, with easy access to actions like closing the view or perhaps interacting further (e.g., playing audio/video).
Accessibility: Accessibility must be considered throughout development. This includes support for Dynamic Type, providing meaningful labels for VoiceOver on all interactive elements, and potentially offering alternative text descriptions for visual memories or transcriptions for audio memories where feasible.
SwiftUI <-> RealityKit Communication: Managing the flow of data and events between the SwiftUI controls and the RealityKit scene is a key architectural consideration. A coordinator class associated with the UIViewRepresentable is typically used for this. For example:

SwiftUI to RealityKit: Changes in the SwiftUI joystick's state (via a ViewModel) trigger calls to the coordinator, which then updates the transform of the avatar Entity within the ARView's scene.
RealityKit to SwiftUI: Gestures recognized by the ARView (like taps on memory cues or swipes for camera control) trigger delegate methods or callbacks implemented by the coordinator. The coordinator then updates state variables in the SwiftUI ViewModel, causing the SwiftUI UI to react accordingly (e.g., showing the full memory view, updating a camera orientation indicator).


4. Technical ArchitectureThis section outlines the proposed technical architecture for Mosaic, covering the frontend, AR/spatial systems, backend services, and data management strategies.4.1. Frontend Architecture (SwiftUI, RealityKit/ARKit Integration)
Architectural Pattern: The Model-View-ViewModel (MVVM) pattern is recommended for organizing the SwiftUI codebase. This pattern promotes separation of concerns, enhances testability, and works well with SwiftUI's declarative nature.5

Models: Plain Swift structs or classes representing the application's data (e.g., SpatialScan, MemoryPost, UserProfile). These models should be independent of the UI.
Views: SwiftUI views responsible for presenting the UI. This includes custom controls like the joystick 16 and containers for the ARView. Views should be lightweight and primarily driven by state provided by ViewModels.
ViewModels: Classes that prepare and provide data for the Views, handle user input logic, and interact with underlying services (e.g., AR services, network services, data storage). They expose state using @Published properties that Views can bind to.


SwiftUI & RealityKit Integration:

The primary rendering surface for AR and 3D content will be an ARView instance from RealityKit.
This ARView will be embedded within the SwiftUI view hierarchy using a custom UIViewRepresentable struct.19
A Coordinator class, part of the UIViewRepresentable implementation, will act as the bridge for handling delegate callbacks from ARView (e.g., gesture recognizers, ARSessionDelegate methods) and translating actions from SwiftUI (e.g., joystick movements) into commands for the RealityKit scene (e.g., updating entity transforms). SwiftUI views will communicate with the coordinator indirectly via the ViewModel and bindings passed to the UIViewRepresentable.


ARKit Foundation: ARKit provides the core tracking and environmental understanding. An ARSession will be configured and run, likely managed implicitly by the ARView or explicitly via the coordinator if finer control is needed (e.g., for managing ARWorldMap data 10). The configuration will likely be ARWorldTrackingConfiguration to enable plane detection and world mapping.
RealityKit Scene Graph: The ARView's scene will contain:

An AnchorEntity representing the loaded RoomPlan scan (or anchored to the ARWorldMap origin).
ModelEntity instances representing the scanned room geometry (loaded from USDZ).
An Entity representing the user's avatar in the 2.5D view.
ModelEntity instances representing the "floating screens" or visual cues for memories.
Camera entities (either default perspective or potentially an OrthographicCameraComponent 17 for the 2.5D view).


State Management: SwiftUI's built-in state management (@State, @StateObject, @ObservedObject, @EnvironmentObject) will be used within the MVVM pattern to manage UI state and data flow. Careful consideration is needed for synchronizing state between the SwiftUI domain and the RealityKit scene graph via the ViewModels and Coordinator.
Table 3: Technology Stack Summary
CategoryTechnology/FrameworkReason for Choice/Key RoleUI FrameworkSwiftUIModern, declarative UI for iOS; programmatic approach; good integration with MVVM.5AR/3D RenderingRealityKitHigh-level framework for AR/3D rendering, animations, physics, spatial audio; built on ARKit; supports USDZ; custom shaders/materials for effects.6AR Tracking/WorldARKitFoundational AR capabilities: world tracking, plane detection, camera access, ARWorldMap persistence 9, ARCamera data.3Room ScanningRoomPlanApple's API for easy 3D interior scanning; identifies layout, objects; outputs USDZ/JSON.1Backend ServiceCloudKit (Recommended)Deep iOS integration, automatic iCloud auth 21, generous storage scaling with users 22, suitable for AR assets & structured data.3D Model FormatUSDZStandard format for AR on Apple platforms; efficient; supported by RealityKit and RoomPlan export.1Scan Metadata FormatJSONStandard, human-readable format for storing parametric data from RoomPlan (object categories, dimensions, etc.).7Architecture PatternMVVMPromotes separation of concerns, testability, and works well with SwiftUI's state management.
4.2. AR/Spatial SystemsThis subsection details the core AR and spatial computing components.
RoomPlan Data Handling:

The scanning process will be managed using RoomCaptureSession and its delegate (RoomCaptureSessionDelegate) to monitor progress and receive results.1 The RoomCaptureView provides the UI.
Upon completion, the CapturedRoom data structure is obtained. This contains arrays of identified walls, doors, windows, and objects.12
This CapturedRoom object will be processed:

Exported to a USDZ file using RoomBuilder for visualization in RealityKit.1
Encoded into a JSON object for storing structured metadata (object categories, attributes, dimensions, transforms).7 Both files will be stored (locally and/or backend) and associated with the space.




Object Interaction & Memory Anchoring:

Anchoring to RoomPlan Objects: When a user selects an object identified by RoomPlan (e.g., tapping its representation in the view), the memory will be associated with that object's unique identifier (CapturedRoom.Object.identifier) stored in the JSON metadata. The memory's position might be initially set relative to the object's bounding box center or a predefined point.
Anchoring to Locations: When a user selects an arbitrary point on a surface, a raycast is performed from the screen touch point into the RealityKit scene. The ARView's raycast(from:allowing:alignment:) method (or similar scene-based raycasting) will be used to find the 3D intersection point with the loaded RoomPlan USDZ mesh.13 This precise SIMD3<Float> world coordinate becomes the memory's anchor.
Memory Anchor Data: The memory record will store either the objectIdentifier (if linked to a RoomPlan object) or the precise SIMD3<Float> anchor coordinate, along with the necessary transform information (position, rotation, scale) relative to the space's origin.


Creator Viewpoint Management:

Capture: During memory creation confirmation, the ARCamera.transform property (a simd_float4x4 matrix) is read from the current ARFrame's camera object.3
Storage: This 4x4 matrix needs to be serialized and stored alongside the memory data in the backend. It can be stored as an array of 16 floats or in a format supported by the chosen backend (e.g., potentially as Data in CloudKit).
Application: When an Explorer views the memory in full, the application retrieves the stored matrix. This matrix is then applied to the transform property of the RealityKit camera entity responsible for the Explorer's view, temporarily overriding their current sand-table perspective. Smooth transitions (e.g., using RealityKit animations) into and out of this viewpoint are desirable. Coordinate system consistency between the creation session and the exploration session is crucial, which ARWorldMap helps maintain.


Persistence & Relocalization Strategy (ARWorldMap):

Necessity: To allow users to revisit their own scanned spaces later or to enable shared experiences where multiple users see memories anchored correctly in the same physical space, ARKit's world-tracking state must be saved and reloaded. ARWorldMap is Apple's mechanism for this.9
Saving: After a successful RoomPlan scan (especially one intended for sharing or persistent personal use), the app should obtain the current ARWorldMap from the ARSession (session.getCurrentWorldMap(completionHandler:)). This requires the worldMappingStatus to be .mapped or .extending.10 The obtained ARWorldMap object, which encapsulates spatial mapping data and existing anchors, is then serialized (e.g., using NSKeyedArchiver.archivedData(withRootObject:requiringSecureCoding:)) into Data. This data blob is uploaded to the backend and associated with the specific Space record.
Loading: When a user loads a space that has an associated ARWorldMap, the app downloads the serialized Data. It deserializes this back into an ARWorldMap object (e.g., using NSKeyedUnarchiver.unarchivedObject(ofClass:from:)). A new ARWorldTrackingConfiguration is created, and its initialWorldMap property is set to the loaded map. The ARSession is then run with this configuration.10
Relocalization: ARKit attempts to match the features in the loaded ARWorldMap with the current physical environment sensed by the device. This process ("relocalization") aligns the current session's coordinate system with the one saved in the map. Success is not guaranteed and depends on environmental consistency and the user being in a previously mapped area.
UX Guidance: Because relocalization can fail or take time, the UI must guide the user. Displaying a snapshot image captured when the map was saved (potentially stored as a custom anchor within the ARWorldMap itself, as demonstrated in Apple's sample code 10) can help the user orient their device correctly. Text prompts ("Move your device slowly," "Point towards the area shown") are also necessary. For public spaces, associating the ARWorldMap with a specific QR code placed physically in the environment can provide a reliable starting point for relocalization.
Relationship with RoomPlan: ARWorldMap and the RoomPlan USDZ model work together. The ARWorldMap re-establishes the correct real-world coordinate system. The USDZ model provides the visual geometry within that coordinate system. Memories are anchored relative to this coordinate system, ensuring they appear in the correct physical location once relocalization succeeds.


4.3. Backend ArchitectureA robust backend is essential for storing data, managing users, and enabling sharing.

Core Responsibilities:

User authentication and profile management.
Storage of Space data: metadata, RoomPlan JSON, USDZ file references, ARWorldMap data references.
Storage of Memory data: metadata (creator, type, privacy, transforms), text content, references to multimedia files.
Storage of large binary assets: USDZ files, ARWorldMap data blobs, photos, video clips, audio clips.
Management and enforcement of sharing permissions and privacy settings.
(Future) Push notifications for updates in shared spaces.



Data Models (Conceptual):
Table 4: Data Model Overview

Entity NameKey Attributes (with types)RelationshipsNotesUseruserID (String/UUID), iCloudRecordID (if CloudKit), displayName (String), profileImageURL (String, optional)- Has many Space records (as owner)<br>- Has many Memory records (as creator)Represents an authenticated user of the app.SpacespaceID (String/UUID), ownerUserID (Ref to User), name (String), description (String, optional), roomPlanJSONURL (String/AssetRef), usdzModelURL (String/AssetRef), arWorldMapDataURL (String/AssetRef, optional), thumbnailURL (String, optional), isPublicLinkShared (Bool), creationDate (Date)- Belongs to one User (owner)<br>- Has many Memory recordsRepresents a single scanned environment. URLs point to assets in cloud storage.MemorymemoryID (String/UUID), spaceID (Ref to Space), creatorUserID (Ref to User), contentType (Enum: Text, Image, Video, Audio), contentData (String for Text, URL/AssetRef for media), anchorTransform (Matrix/Components or ObjectRef+Offset), creatorViewpointTransform (Matrix/Components), presentationTransform (Matrix/Components for visual media), privacySetting (Enum: Private, PublicInSpace), timestamp (Date), objectAnchorID (String, optional)- Belongs to one Space<br>- Belongs to one User (creator)Represents a single memory posting. Transforms store placement and viewpoint data. objectAnchorID links to a RoomPlan object if applicable.

API Design: If a custom backend were built, RESTful or GraphQL APIs would provide endpoints for standard CRUD (Create, Read, Update, Delete) operations on Users, Spaces, and Memories. Separate mechanisms (e.g., pre-signed URLs for S3/GCS, or direct asset handling in CloudKit) would be needed for efficient upload/download of large binary files.


Backend Service Recommendation: CloudKit vs. Firebase
A careful evaluation of backend services is needed, primarily comparing Apple's CloudKit with Google's Firebase.
Table 5: Backend Service Comparison (CloudKit vs. Firebase)


CriterionCloudKit AssessmentFirebase Assessment (Firestore + Storage)Pricing/ScalingGenerous free tier for public/private DB storage and requests. Asset storage scales significantly with active users (potentially petabytes).22 Cost-effective for asset-heavy apps within Apple ecosystem. Limits exist but may be flexible for assets.22Free tier for Firestore is limited (1GiB storage, 50k reads/day).24 Pricing based on reads, writes, deletes, storage, bandwidth.25 Can become expensive at scale, especially with high operation counts or large storage needs.26 Predictable but potentially higher cost for asset storage.Ease of UseCan have a steeper learning curve for some developers.26 Requires understanding CloudKit concepts (containers, scopes, record types, zones). Native framework integration.Often considered easier to get started with.26 Well-documented APIs, large community support. Flexible NoSQL (Firestore) or JSON tree (Realtime DB) models.25iOS IntegrationExcellent. Deep integration with iOS/macOS/etc. Native Swift APIs. Seamless iCloud user authentication.21Good SDKs for iOS. Requires separate authentication setup (Firebase Auth).Asset StorageDesigned to handle assets (CKAsset). Scales very well with user base.22 Potential individual asset size limits need verification.23Firebase Storage (built on Google Cloud Storage) handles large files well. Pricing is per GB stored and transferred. Doesn't scale freely with users like CloudKit asset storage.User AuthenticationAutomatic via iCloud.21 Simplifies user onboarding significantly for logged-in iCloud users.Requires Firebase Authentication setup (Email/Pass, Google, Apple Sign-In, etc.). More flexible for cross-platform but more setup work.Cross-platformPrimarily Apple ecosystem. Mac Catalyst apps can access some parts.1 No native support for Android/Web backends.Excellent. SDKs for iOS, Android, Web, Unity, C++, etc. Ideal if future cross-platform expansion is planned.Real-time SyncSupports subscriptions for push notifications on data changes. Can build real-time features but might require more manual setup than Firebase RTDB.Firebase Realtime Database is built for low-latency sync. Firestore also offers real-time listeners.
**Recommendation:** For the Mosaic application, **CloudKit is the recommended backend service.**
*   **Justification:** The primary drivers for this recommendation are the seamless iCloud authentication [21] (simplifying user onboarding on iOS) and, most importantly, the **highly advantageous storage scaling model for assets**.[22] An application storing numerous potentially large USDZ scans, ARWorldMap data, and multimedia files benefits significantly from storage limits that grow with the user base, likely resulting in lower operational costs compared to Firebase's per-GB storage pricing. While potential individual asset size limits [23] need investigation during development, the overall architecture seems well-suited. The iOS-only nature of the app negates Firebase's cross-platform advantage. The perceived steeper learning curve of CloudKit is considered a manageable trade-off for the long-term cost and integration benefits within the Apple ecosystem.
4.4. Data Management
Data Formats:

3D Scans: USDZ for rendering 1, JSON for metadata.7
AR Persistence: Serialized Data blob for ARWorldMap.10
Memories: Metadata stored in CloudKit Records (or Firestore Documents). Text stored directly. Multimedia stored as CKAsset in CloudKit (or files in Firebase Storage), referenced by the memory record. Native file types (JPEG, PNG, HEIC, MP4, M4A) preferred.


Storage Strategy:

Cloud (Authoritative): CloudKit Public and Private Databases will store all user, space, and memory data, including assets. Public Database for shared space metadata and public memories; Private Database for user-specific data and private memories/spaces.
Local Cache: Implement on-device caching (e.g., using Core Data or simply files in the app's container) for recently accessed Space data (USDZ, JSON, ARWorldMap) and associated Memory data. This improves performance for frequently accessed spaces and enables limited offline viewing. Cache eviction policies will be needed.


Data Synchronization:

Utilize CloudKit's built-in mechanisms for fetching and saving records (CKDatabase operations).
Implement logic to fetch updates from CloudKit periodically or based on user actions.
Use CloudKit subscriptions (CKSubscription) to receive push notifications when relevant data changes in the cloud (e.g., a new memory added to a shared space the user follows), allowing the app to refresh data proactively.
Conflict resolution for simultaneous edits (rare for memories initially, but possible) will default to "last write wins" for MVP. More sophisticated strategies (e.g., using record change tags) could be implemented later if needed.


5. Development RoadmapThis roadmap breaks down the development process into logical phases, focusing on delivering incremental value and managing scope. Timelines are explicitly excluded; the focus is on the sequence and content of work.5.1. Phase 1: Foundational MVP (Core Scan, Basic Memory Creation & Local 3D View)
Goal: Establish the absolute core functionality in a local-only context. Validate the scan-place-viewpoint loop. Provide a tangible, interactive baseline as quickly as possible.
Key Features & Tasks:

Basic SwiftUI App Shell: Set up project structure, basic navigation, placeholder views.
RoomPlan Integration: Implement the RoomCaptureView flow.1 Process results into USDZ/JSON.
Local Scan Storage: Save/load USDZ and JSON files to the device's local storage. Implement a simple list view to browse saved scans.
Basic RealityKit Viewer: Create a UIViewRepresentable for ARView.19 Load a selected USDZ scan into the ARView. Implement basic camera controls (orbit, pan, zoom) for viewing the model in 3D (not 2.5D yet).
Local Memory Model & Storage: Define a simple Memory struct/class for text-only memories. Use Core Data or property lists to store memory metadata (text content, anchor point) locally, associated with a saved scan.
3D Point Placement: Implement raycasting 13 in the RealityKit viewer to allow tapping on a surface to get a 3D anchor point. Save this point with the memory.
Viewpoint Capture & Recall:

Capture: When saving a memory, get the ARCamera.transform 3 and store it (serialized) with the local memory data.
Recall: Add a button in the memory detail view (e.g., "View from Creator's Perspective"). When tapped, set the RealityKit viewer's camera transform to the stored matrix.




Rationale: This phase focuses entirely on the client-side AR mechanics without the complexity of backend integration or the specialized 2.5D UI. It delivers a core interactive loop quickly for testing and iteration.
5.2. Phase 2: Enhanced Interaction, Backend Integration & Basic 2.5D View
Goal: Introduce cloud persistence, basic sharing, the initial 2.5D exploration view, and support for photo memories.
Key Features & Tasks:

Backend Setup (CloudKit): Configure CloudKit container, define record types (User, Space, Memory).23 Implement basic iCloud user authentication.21
Scan Data Sync: Implement logic to upload scans (USDZ, JSON) to CloudKit CKAssets / Records. Implement downloading/fetching spaces.
Memory Data Sync (Text): Sync text memories created locally (from Phase 1) or newly created memories with CloudKit.
Basic 2.5D Sand-Table View:

Implement the "upper window" ARView displaying the scan from a fixed top-down or angled perspective (prototype OrthographicCameraComponent 17 vs. constrained perspective).
Add a simple avatar Entity to the scene.
Implement the SwiftUI joystick 16 in the "lower window" to control avatar translation.
Implement swipe gesture on the ARView to control camera horizontal orbit.


Memory Cues in 2.5D: Display simple visual markers (e.g., colored dots or basic icons) at memory anchor locations in the 2.5D view.
Viewpoint Adoption in 2.5D: Implement the logic for tapping a memory cue to transition the 2.5D camera to the creator's stored viewpoint transform and back.
Photo Memories: Extend the memory creation flow to support capturing/selecting photos. Upload photos as CKAssets. Display photos in the memory detail view.
Memory "Floating Screen" (Basic): In the memory creation view (still likely 3D for placement), allow basic positioning/orientation of a plane/quad that will represent the photo memory. Store this presentation transform. Display these floating screens in the 2.5D view when exploring.
Basic Space Sharing: Implement functionality to generate a shareable link for a space (using CloudKit sharing features or custom logic). Allow users to open a space via a link.


5.3. Phase 3: Refinement, Sharing, Full 2.5D Experience & Advanced Features
Goal: Deliver the full, polished user experience envisioned in the app flow, including advanced 2.5D interactions, robust persistence, and full multimedia support.
Key Features & Tasks:

Full 2.5D Experience Enhancements:

Implement refined visual cues (light pulses, atmospheric effects using RealityKit materials/shaders 6).
Implement snippet/preview display on proximity/interaction.
Implement atmospheric/mood shifts based on memory content/tags.
Implement persistence of discovered memory states.


ARWorldMap Integration:

Implement saving ARWorldMap data to CloudKit alongside scans.10
Implement loading initialWorldMap for improved relocalization when revisiting spaces or accessing shared spaces.9
Develop UI guidance for the relocalization process (e.g., snapshot display 10).


Refined Sharing & Privacy:

Implement memory-level privacy settings (Private, Public within Space) enforcement via CloudKit queries and security rules.
(Optional) Add anonymity option for public memories.


Full Multimedia Support: Add support for Video and Audio memory types (capture, upload, playback).
Object-Based Memory Attachment: Refine the UX for selecting RoomPlan-identified objects 11 during memory placement and associating the memory correctly.
Height Specificity UI: Add explicit UI controls during placement to define the memory anchor's height accurately.
Enhanced Memory Volume Placement: Improve the 3D controls for positioning and orienting the "floating screens" for visual media.


5.4. Future Enhancements (Post-Initial Launch)
Advanced multi-session geometric refinement for scans (dependent on future RoomPlan/ARKit capabilities).
Sharing memories/spaces with specific users or groups (requires significant backend work: ACLs, user relationships).
In-app map/directory for discovering nearby or featured public spaces.
Advanced search, filtering, and sorting of memories within a space.
Real-time collaborative features (e.g., multiple users adding memories simultaneously in a shared AR session).
Integration with other platforms or data sources.
Table 6: Phased Development Roadmap
PhasePhase GoalKey Features/TasksPhase 1: Foundational MVPValidate core scan-place-viewpoint loop locally.Basic SwiftUI Shell, RoomPlan Scan & Local Save (USDZ/JSON), Basic 3D RealityKit Viewer, Local Text Memory Storage, 3D Point Placement (Raycast), Creator Viewpoint Capture & Recall (Local 3D View).Phase 2: Backend & Basic 2.5DIntroduce cloud persistence, basic sharing, initial 2.5D view, photo memories.CloudKit Setup & Auth, Scan/Memory Sync (Text), Basic 2.5D View (Avatar, Joystick, Orbit Cam), 2.5D Memory Cues & Viewpoint Adoption, Photo Memory Support, Basic Floating Screen Placement, Basic Link Sharing.Phase 3: Polished ExperienceDeliver full 2.5D UX, robust persistence, full media, refined interactions.Enhanced 2.5D Visuals (Cues, Previews, Atmosphere), ARWorldMap Save/Load & Relocalization UI 10, Memory Privacy Enforcement, Video/Audio Memory Support, Object-Based Memory Attachment Refinement 11, Height Specificity UI, Enhanced Volume Placement Controls.Future EnhancementsExtend functionality beyond initial scope.Multi-session scan refinement, User-to-user sharing, Public space directory, Advanced search/filtering, Real-time collaboration.
6. Logical Dependency ChainThis section outlines the logical order in which features should be developed, ensuring foundational components are built before dependent features.

1. Foundation First (Core App & Local AR Setup):

Project Setup & SwiftUI Shell: Establish the Xcode project, basic app structure (MVVM), navigation flow, and placeholder views for main sections.
RoomPlan Integration (Local): Implement the scanning flow using RoomCaptureView.1 Process the CapturedRoom data into USDZ and JSON formats.7 Implement local file saving and loading for these scan artifacts.
Basic RealityKit Scene: Set up the UIViewRepresentable for ARView.19 Implement loading a saved USDZ file into the ARView scene. Implement basic 3D camera controls (orbit/pan/zoom) for initial viewing.



2. Getting to Usable/Visible Front-End (MVP - Phase 1 Focus):4.  Local Memory Model & Storage: Define the data structure for a simple text memory. Implement local persistence (e.g., Core Data, Files) to store these memories associated with their respective local scans.5.  3D Memory Placement: Implement raycasting in the RealityKit view 13 to allow users to tap on the loaded scan mesh and get a 3D coordinate. Save this coordinate as the memory's anchor.6.  Creator Viewpoint Capture & Recall: Implement capturing the ARCamera.transform 3 when a memory is saved locally. Add the UI element and logic to set the RealityKit view's camera transform to the saved viewpoint matrix when viewing a memory's details.


3. Building Upon (Phase 2 & 3 Dependencies):7.  Backend Integration - Core: Set up CloudKit 23, implement user authentication via iCloud 21, define basic User, Space, Memory record types.8.  Scan Data Cloud Sync: Implement upload/download logic for scan data (USDZ, JSON) between the local device and CloudKit.9.  Initial 2.5D View Implementation: Create the 2.5D camera setup (Orthographic 17 or constrained Perspective). Implement the avatar, joystick movement 16, and swipe-to-orbit camera control. Display basic memory cues.10. Memory Cloud Sync (Text): Migrate text memory storage from local to CloudKit. Ensure viewpoint data is also synced.11. Viewpoint Adoption in 2.5D: Connect the memory cue interaction in the 2.5D view to the viewpoint transition logic.12. Multimedia Support (Photos): Extend data models and CloudKit integration to handle photo uploads (CKAsset) and downloads. Implement image capture/selection UI. Add floating screen representation.13. ARWorldMap Persistence: Integrate ARWorldMap saving during scan finalization and loading during space access.9 Implement relocalization UI.14. Sharing Logic (Link-Based): Implement CloudKit sharing features or custom logic to generate and handle space access via links. Filter memories based on "PublicInSpace" privacy.15. Advanced 2.5D Visuals: Implement refined cues, previews, and atmospheric effects.616. Remaining Features: Add Video/Audio support, refine object attachment 11, height specificity, etc.


Pacing and Atomicity: This dependency chain prioritizes getting a functional, local AR experience (scan, place text memory, recall viewpoint in 3D) working first. This provides the fastest path to validating the core interaction model. Subsequent steps layer on complexity incrementally: backend sync, the specialized 2.5D UI, richer media types, and robust persistence via ARWorldMap. Each numbered step represents a logical block of work that builds upon the previous ones, facilitating iterative development and testing.

7. Risks and MitigationsThis section identifies potential risks during the development of Mosaic and outlines mitigation strategies.7.1. Technical Challenges
Risk 1: RoomPlan Accuracy & Object Recognition Limitations

Description: RoomPlan scans may contain geometric inaccuracies, fail to capture certain details, or misclassify objects. Furthermore, object recognition provides categories and bounding boxes, not precise instance geometry 11, which might not meet user expectations for attaching memories to very specific parts of an object.
Likelihood: Medium
Impact: Medium (Can lead to user frustration with placement accuracy or object interaction).
Mitigation: Set realistic user expectations through UI text and tutorials about RoomPlan's capabilities. Design the memory placement UX to clearly support both coarse object selection (based on bounding boxes) and precise location pinning via raycasting. Prioritize robust raycasting for reliable placement on any surface. Accept categorical object linking for MVP. Continuously evaluate RoomPlan updates for improvements.


Risk 2: ARWorldMap Relocalization Reliability

Description: Reloading a space using a saved ARWorldMap requires ARKit to successfully relocalize within the physical environment. This can fail due to environmental changes (lighting, moved furniture), insufficient mapped features, or the user starting in an unrecognized area.9 Failed relocalization means memories won't appear correctly anchored.
Likelihood: High (Especially for shared spaces or spaces revisited after a long time).
Impact: High (Core functionality of revisiting/sharing spaces breaks down).
Mitigation: Implement comprehensive user guidance during the relocalization phase (e.g., displaying the saved snapshot 10, clear text prompts, progress indicators). Strongly consider using physical markers (like QR codes) associated with specific ARWorldMaps for public spaces to provide reliable starting points. Test relocalization extensively in diverse conditions. Have fallback mechanisms (e.g., allow browsing memories without spatial anchoring if relocalization fails repeatedly).


Risk 3: Performance with Complex Scenes/Many Memories

Description: Rendering complex RoomPlan USDZ models combined with potentially numerous memory visualizations (floating screens, cues, effects) in RealityKit could strain device resources (CPU/GPU), leading to low frame rates, overheating, or excessive battery drain, especially on older supported devices.
Likelihood: Medium
Impact: Medium (Degraded user experience, potential app instability).
Mitigation: Optimize USDZ assets generated from RoomPlan where possible. Implement view frustum culling and potentially Level of Detail (LOD) for memory visualizations (only render/show detail for nearby/visible memories). Use efficient rendering techniques in RealityKit (e.g., instancing if applicable). Profile performance regularly on target devices using Xcode Instruments. Leverage RealityKit's performance optimizations 6 but apply best practices diligently.


Risk 4: 2.5D View Camera Control Complexity

Description: Implementing the specific camera behavior for the 2.5D view (avatar-following translation, fixed pitch, manual swipe-to-orbit) requires careful coordination between SwiftUI controls, user gestures, and RealityKit camera manipulation. Achieving a smooth, intuitive feel can be challenging.
Likelihood: Medium
Impact: Medium (Poor camera control can make exploration frustrating).
Mitigation: Prototype the camera controller early in Phase 2. Experiment with both OrthographicCameraComponent 17 and constrained PerspectiveCameraComponent to determine the best fit for the desired UX. Iterate based on user feedback. If the exact specification proves overly complex, consider slightly simplifying the constraints for the initial implementation.


7.2. Scope Management
Risk 5: Feature Creep

Description: The potential exists to expand scope beyond the defined phases, particularly regarding complex sharing models (user-to-user, groups), advanced AR interactions, or extensive backend features not essential for the core vision.
Likelihood: Medium
Impact: High (Delayed delivery, budget overruns, loss of focus).
Mitigation: Strictly adhere to the phased development roadmap (Table 6). Maintain a clear definition of the MVP (Phase 1) and subsequent phase goals. Use a backlog for "Future Enhancements" and resist pulling them into earlier phases without strong justification and impact assessment. Regularly review scope against the roadmap during development.


Risk 6: Defining a Viable MVP

Description: Difficulty in agreeing on or sticking to the minimal feature set required for the initial launch (Phase 1), potentially delaying the ability to get user feedback on core mechanics.
Likelihood: Low (Phase 1 is clearly defined).
Impact: Medium (Slower iteration cycle, delayed validation).
Mitigation: The defined Phase 1 serves as the MVP, focusing on local scan-place-viewpoint validation. Ensure team alignment on this scope. Emphasize the goal of getting the core AR interaction working first before adding layers of complexity.


7.3. Resource & Framework Considerations
Risk 7: Learning Curve for AR/Spatial Frameworks

Description: Team members, particularly newer ones, may require time to become proficient with RoomPlan, ARKit (especially ARWorldMap), and RealityKit concepts and APIs.
Likelihood: Medium (Depends on team experience).
Impact: Medium (Potential impact on development velocity).
Mitigation: Allocate dedicated time for learning, experimentation, and reviewing Apple's documentation 1 and sample code. Encourage pair programming or knowledge sharing. Start with simpler features within each framework (as outlined in the dependency chain) and build complexity gradually.


Risk 8: API Stability & Future Changes

Description: While targeting 2025 with Xcode 16.3 suggests using relatively mature APIs, Apple frequently introduces updates or deprecations with new iOS versions or WWDC announcements.
Likelihood: Low (For core functionalities).
Impact: Low (Usually requires manageable code updates).
Mitigation: Stay informed about Apple's developer updates. Write modular code to isolate dependencies on specific framework APIs where possible, easing future refactoring if needed. Focus on APIs documented as stable.


Risk 9: CloudKit Limitations or Unexpected Costs

Description: While recommended, CloudKit has documented limits (e.g., request rates, potentially individual asset sizes 23). If the app generates unexpected traffic patterns or hits undocumented limits, or if the generous asset scaling 22 proves less flexible than anticipated for specific large file types, issues could arise.
Likelihood: Low-Medium
Impact: Medium (May require architectural changes or unexpected costs if limits are hit hard).
Mitigation: Thoroughly investigate CloudKit's practical limits for large CKAssets (USDZ, ARWorldMap data) early in development (Phase 2). Implement efficient data fetching strategies (batching, caching) to stay within request quotas. Monitor CloudKit dashboard telemetry closely upon launch. Design data models considering potential limits.


Table 7: Risk Assessment Matrix
Risk IDRisk DescriptionLikelihoodImpactMitigation StrategyR1RoomPlan Accuracy/Object Recognition LimitsMediumMediumSet user expectations; Robust raycasting fallback; Accept categorical linking 11; Monitor RoomPlan updates.R2ARWorldMap Relocalization ReliabilityHighHighComprehensive user guidance (snapshots 10, prompts); QR code integration for public spaces; Test extensively; Fallback non-spatial view.R3Performance with Complex Scenes/MemoriesMediumMediumOptimize assets; Culling/LOD for memories; Efficient RealityKit usage 6; Regular profiling.R42.5D Camera Control ComplexityMediumMediumEarly prototyping; Test Orthographic 17 vs. Constrained Perspective; Iterate on feel; Simplify if needed for MVP.R5Feature CreepMediumHighStrict adherence to phased roadmap (Table 6); Clear MVP definition; Backlog management.R6Defining Viable MVPLowMediumPhase 1 is the defined MVP; Ensure team alignment; Focus on core AR loop first.R7Learning Curve for AR FrameworksMediumMediumAllocate learning time; Use Apple docs/samples 1; Incremental feature complexity.R8API Stability & Future ChangesLowLowStay updated; Modular code design; Focus on stable APIs.R9CloudKit Limitations/CostsLow-MediumMediumInvestigate large asset limits early 22; Efficient data fetching; Monitor telemetry; Design for limits.
8. Appendix8.1. Key Research Findings Summary
RoomPlan: Effectively scans interiors, identifies common objects categorically (not instance-specific geometry), and outputs USDZ/JSON.1 Requires RoomCaptureView for guided scanning.1
ARKit/RealityKit: RealityKit is the high-level framework for rendering and interaction, built on ARKit.6 ARCamera.transform provides viewpoint data.3 ARWorldMap enables persistence and relocalization but requires careful handling and user guidance.9 Raycasting is used for precise point selection.13 RealityKit offers advanced rendering features suitable for atmospheric effects.6
SwiftUI Integration: UIViewRepresentable is the standard way to embed ARView in SwiftUI.19 MVVM pattern is recommended for structure.5 Careful state management between SwiftUI and RealityKit is needed.
Backend (CloudKit Recommended): CloudKit offers strong iOS integration, iCloud auth 21, and potentially very cost-effective asset storage scaling based on user count 22, outweighing potential limits 23 and perceived ease-of-use advantages of Firebase 24 for this specific iOS-centric, asset-heavy application.
Controls/Interaction: SwiftUI provides tools for custom controls like joysticks.16 RealityKit handles 3D interactions and camera controls, including orthographic options.17
8.2. Detailed Data Model Schemas (CloudKit Focus)(This expands on Table 4, assuming CloudKit)
Record Type: Users (Private Database)

recordName: (System generated, often derived from iCloud User Record ID)
displayName: String
profileImage: CKAsset (Optional)


Record Type: Spaces (Public or Private Database, depending on sharing)

recordName: UUID String (Generated by app)
ownerRef: CKRecord.Reference (Links to Users record, DELETE_SELF action)
name: String
description: String (Optional)
roomPlanJSON: CKAsset
usdzModel: CKAsset
arWorldMapData: CKAsset (Optional)
thumbnail: CKAsset (Optional)
isPublicLinkShared: Int64 (0=false, 1=true) - Use Int for potential indexing
creationDate: Date
(Indexes recommended on ownerRef, creationDate)


Record Type: Memories (Public or Private Database, depending on Space & Memory privacy)

recordName: UUID String (Generated by app)
spaceRef: CKRecord.Reference (Links to Spaces record, DELETE_SELF action)
creatorRef: CKRecord.Reference (Links to Users record, NONE action - keep memory even if user deleted?)
contentType: Int64 (Enum mapping: 0=Text, 1=Image, 2=Video, 3=Audio)
contentText: String (Only if contentType=Text)
contentMedia: CKAsset (Only if contentType=Image/Video/Audio)
anchorPositionX, Y, Z: Double (If location-based anchor)
anchorObjectIdentifier: String (If object-based anchor)
creatorViewpointTransform: Data (Serialized simd_float4x4 matrix)
presentationTransform: Data (Serialized simd_float4x4 matrix, for visual media)
privacySetting: Int64 (Enum mapping: 0=Private, 1=PublicInSpace)
isAnonymous: Int64 (0=false, 1=true, if PublicInSpace)
timestamp: Date
(Indexes recommended on spaceRef, creatorRef, timestamp, privacySetting)


8.3. Draft API Endpoint Definitions (N/A for CloudKit)CloudKit uses direct framework APIs (e.g., CKDatabase.fetch, CKDatabase.save, CKQueryOperation) rather than traditional REST/GraphQL endpoints. Interactions involve creating CKRecord objects, setting fields, and performing operations on a CKDatabase instance. Asset uploads/downloads are handled via CKAsset properties. Queries use NSPredicate and CKQuery. Subscriptions use CKQuerySubscription.8.4. Key Apple Documentation References
RoomPlan: https://developer.apple.com/documentation/roomplan 1
ARKit: https://developer.apple.com/documentation/arkit

ARWorldMap: https://developer.apple.com/documentation/arkit/arworldmap 27
Saving and Loading World Data: https://developer.apple.com/documentation/arkit/saving_and_loading_world_data 10
ARCamera: https://developer.apple.com/documentation/arkit/arcamera 28


RealityKit: https://developer.apple.com/documentation/realitykit 29

ARView: https://developer.apple.com/documentation/realitykit/arview 30
Cameras (Perspective/Orthographic): https://developer.apple.com/documentation/realitykit/scene-content-lights-and-cameras 18


SwiftUI: https://developer.apple.com/documentation/swiftui
CloudKit: https://developer.apple.com/documentation/cloudkit
9. ConclusionThis Product Requirements Document provides a comprehensive plan for developing the Mosaic iOS application. By leveraging SwiftUI for the interface, RoomPlan for spatial capture, and ARKit/RealityKit for anchoring and visualizing memories, Mosaic aims to deliver a unique and evocative user experience centered around spatially contextualized digital memories.The core differentiators—precise memory placement including creator viewpoint capture, and the novel 2.5D sand-table exploration mode—present exciting UX opportunities but also require careful technical implementation. The recommended architecture emphasizes MVVM for SwiftUI, UIViewRepresentable for RealityKit integration, and CloudKit for backend services, capitalizing on its iOS integration and user-based storage scaling for AR assets.The phased roadmap prioritizes delivering core AR functionality locally first (MVP), followed by backend integration, the 2.5D view, and feature refinement. This iterative approach, combined with proactive risk mitigation focusing on potential challenges in AR tracking, relocalization, and performance, provides a structured path forward.Successful execution according to this plan should result in a compelling application that uniquely bridges the gap between digital memories and the physical world, fostering deeper connections to places and shared experiences. Continuous attention to performance, usability, and the specific technical challenges outlined will be crucial throughout the development lifecycle.